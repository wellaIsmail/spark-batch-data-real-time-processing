{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16813f4-814d-4840-8bff-ce200a0ef690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, regexp_replace, sum, when, sum as spark_sum, avg, rank, desc\n",
    "from pymongo import MongoClient\n",
    "#from pyspark.sql.window importÂ Window\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d196cbf2-acc4-4921-9927-ba7c2254a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b212ba0b-891b-41cd-83a0-2434f97b01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Real-time purchasing habits of high-value customers\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c9e2ec-a090-4f7b-9eb4-5d4b7f859ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDFS directories\n",
    "customer_data_dir = \"hdfs://namenode:9000/csv_files/customers/\"\n",
    "product_data_dir = \"hdfs://namenode:9000/csv_files/products/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d153ae1a-12b8-4de9-a923-8634ba4cd9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Item_Purchased: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Purchase_Amount_USD: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Review Rating: string (nullable = true)\n",
      " |-- Subscription Status: string (nullable = true)\n",
      " |-- Shipping Type: string (nullable = true)\n",
      " |-- Discount Applied: string (nullable = true)\n",
      " |-- Promo Code Used: string (nullable = true)\n",
      " |-- Previous Purchases: string (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      " |-- Frequency of Purchases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a static DataFrame to infer the schema for customer data\n",
    "try:\n",
    "    customer_static_df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(customer_data_dir)\n",
    "    customer_static_df.printSchema()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading static data: {e}\")\n",
    "    spark.stop()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d6604f-91b6-4454-8350-eeb60a51dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame for customer data\n",
    "try:\n",
    "    customer_streaming_df = spark.readStream \\\n",
    "        .schema(customer_static_df.schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(customer_data_dir)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating streaming DataFrame: {e}\")\n",
    "    spark.stop()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edf6ed9-978b-40f3-a9b9-2000bd6b4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create MongoClient instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e153671b-5196-4a5a-b7a4-ccceed0dc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection setup\n",
    "try:\n",
    "    #client = MongoClient(mongo_host, mongo_port)\n",
    "    client = MongoClient(\"mongodb://mongodb:27017\")\n",
    "    db = client['Customers']\n",
    "    coll_high_value_customers = db['high_value_customers']\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error connecting to MongoDB: {e}\")\n",
    "    spark.stop()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5547f4e5-5cfe-49ae-8a6b-d538a907fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_high_value_customers(batch_df, batch_id):\n",
    "\n",
    "    try:\n",
    "    # Define the frequency mapping\n",
    "        frequency_mapping = {\n",
    "          \"Weekly\": 5,\n",
    "          \"Bi-Weekly\": 4,\n",
    "          \"Fortnightly\": 4,\n",
    "          \"Monthly\": 3,\n",
    "          \"Quarterly\": 2,\n",
    "          \"Every 3 Months\": 2,\n",
    "          \"Annually\": 1\n",
    "          }\n",
    "              \n",
    "        # Create a column for Frequency_Num based on Frequency of Purchases\n",
    "        batch_df = batch_df.withColumn(\"Frequency_Num\",\n",
    "                                       when(col(\"Frequency of Purchases\") == \"Weekly\", 5)\n",
    "                                       .when(col(\"Frequency of Purchases\") == \"Bi-Weekly\", 4)\n",
    "                                       .when(col(\"Frequency of Purchases\") == \"Fortnightly\", 4)\n",
    "                                       .when(col(\"Frequency of Purchases\") == \"Monthly\", 3)\n",
    "                                       .when(col(\"Frequency of Purchases\") == \"Quarterly\", 2)\n",
    "                                       .when(col(\"Frequency of Purchases\") == \"Every 3 Months\", 2)\n",
    "                                       .when(col(\"Frequency of Purchases\") == \"Annually\", 1)\n",
    "                                       .otherwise(0))\n",
    "        \n",
    "        # Filter high-value customers\n",
    "        high_value_customers = batch_df.filter(\n",
    "            (col(\"Frequency_Num\") > 1) & \n",
    "            (col(\"Purchase_Amount_USD\") > 1) &\n",
    "            (col(\"Previous Purchases\") > 1)\n",
    "        )\n",
    "              \n",
    "        # Group and aggregate the data\n",
    "        customer_agg = high_value_customers.groupBy(\"customer_id\").agg(\n",
    "            spark_sum(\"Purchase_Amount_USD\").alias(\"TotalPurchaseAmount\"),\n",
    "            spark_sum(\"Frequency_Num\").alias(\"TotalFrequency\"),\n",
    "            spark_sum(\"Previous Purchases\").alias(\"TotalPreviousPurchases\")\n",
    "        )\n",
    "        \n",
    "        # Calculate a combined score\n",
    "        customer_agg = customer_agg.withColumn(\n",
    "            \"Combined_Score\",\n",
    "            col(\"TotalPurchaseAmount\") + col(\"TotalFrequency\") + col(\"TotalPreviousPurchases\")\n",
    "        )\n",
    "        \n",
    "        # Sort the data by Combined_Score in descending order and select top 10\n",
    "        top_10_customers = customer_agg.orderBy(col(\"Combined_Score\").desc()).limit(10)\n",
    "        \n",
    "        # Extract top 10 customer IDs\n",
    "        top_10_customer_ids = [row['customer_id'] for row in top_10_customers.collect()]\n",
    "        \n",
    "        # Filter original DataFrame for top 10 customer IDs\n",
    "        top_customers_df = batch_df.filter(col(\"customer_id\").isin(top_10_customer_ids))\n",
    "        \n",
    "        # Analyze their purchasing habits\n",
    "        habits = top_customers_df.groupBy(\"Item_Purchased\", \"Category\", \"Size\", \"Color\", \"Season\", \"Payment Method\", \"Shipping Type\").agg(\n",
    "            spark_sum(\"Purchase_Amount_USD\").alias(\"TotalPurchaseAmount\"),\n",
    "            avg(\"Review Rating\").alias(\"AverageReviewRating\"),\n",
    "            spark_sum(\"Discount Applied\").alias(\"TotalDiscountApplied\"),\n",
    "            spark_sum(\"Previous Purchases\").alias(\"TotalPreviousPurchases\")\n",
    "        ).orderBy(col(\"TotalPurchaseAmount\").desc())\n",
    "        \n",
    "        # Convert the habits DataFrame to a list of dictionaries\n",
    "        habits_list = [row.asDict() for row in habits.collect()]\n",
    "         # Insert into MongoDB if the list is not empty\n",
    "        if habits_list:\n",
    "          coll_high_value_customers.insert_many(habits_list)\n",
    "          habits.show()\n",
    "          logger.info(f\"Batch {batch_id} processed and inserted into MongoDB\")\n",
    "        else:\n",
    "          logger.info(f\"Batch {batch_id} processed but no high-value customers found\")\n",
    "    # Insert into MongoDB (assuming you have established a connection)\n",
    "    # Replace 'db.high_value_customer_habits' with your MongoDB collection\n",
    "   # coll_high_value_customers.insert_many(habits_list)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing batch {batch_id}: {e}\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4214b-8293-4bea-9e5c-691f6abd9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.java_gateway:Callback Server Starting\n",
      "INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 37639)\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "ERROR:__main__:Error processing batch 0: name 'insights_list' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Write the streaming DataFrame to the console using the function\n",
    "try:\n",
    "    query_customer = customer_streaming_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .foreachBatch(process_high_value_customers) \\\n",
    "        .start()\n",
    "    # Await termination\n",
    "    query_customer.awaitTermination()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in streaming query: {e}\")\n",
    "finally:\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
