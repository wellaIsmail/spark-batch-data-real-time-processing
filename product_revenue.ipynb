{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16813f4-814d-4840-8bff-ce200a0ef690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, regexp_replace, sum, when, sum as spark_sum, avg, rank, desc\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql.window import Window\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d196cbf2-acc4-4921-9927-ba7c2254a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b212ba0b-891b-41cd-83a0-2434f97b01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Most popular product categories among different age groups\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c9e2ec-a090-4f7b-9eb4-5d4b7f859ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HDFS directories\n",
    "customer_data_dir = \"hdfs://namenode:9000/csv_files/customers/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d153ae1a-12b8-4de9-a923-8634ba4cd9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Item_Purchased: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Purchase_Amount_USD: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Review Rating: string (nullable = true)\n",
      " |-- Subscription Status: string (nullable = true)\n",
      " |-- Shipping Type: string (nullable = true)\n",
      " |-- Discount Applied: string (nullable = true)\n",
      " |-- Promo Code Used: string (nullable = true)\n",
      " |-- Previous Purchases: string (nullable = true)\n",
      " |-- Payment Method: string (nullable = true)\n",
      " |-- Frequency of Purchases: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a static DataFrame to infer the schema for customer data\n",
    "try:\n",
    "    customer_static_df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(customer_data_dir)\n",
    "    customer_static_df.printSchema()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading static data: {e}\")\n",
    "    spark.stop()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d6604f-91b6-4454-8350-eeb60a51dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame for customer data\n",
    "try:\n",
    "    customer_streaming_df = spark.readStream \\\n",
    "        .schema(customer_static_df.schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(customer_data_dir)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating streaming DataFrame: {e}\")\n",
    "    spark.stop()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e153671b-5196-4a5a-b7a4-ccceed0dc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection setup\n",
    "try:\n",
    "    #client = MongoClient(mongo_host, mongo_port)\n",
    "    client = MongoClient(\"mongodb://mongodb:27017\")\n",
    "    db = client['Products']\n",
    "    coll_high_revenue = db['high_revenue']\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error connecting to MongoDB: {e}\")\n",
    "    spark.stop()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5547f4e5-5cfe-49ae-8a6b-d538a907fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_highest_sales_revenue(batch_df, batch_id):\n",
    "   \n",
    "   try:\n",
    "         # Group by Item_Purchased and calculate total sales revenue\n",
    "        product_sales = batch_df.groupBy(\"Item_Purchased\").agg(\n",
    "             spark_sum(\"Purchase_Amount_USD\").alias(\"TotalSalesRevenue\")\n",
    "        )\n",
    "      \n",
    "    # Sort the results by TotalSalesRevenue in descending order\n",
    "        sorted_product_sales = product_sales.orderBy(desc(\"TotalSalesRevenue\"))\n",
    "    \n",
    "    # Convert the DataFrame to a list of dictionaries\n",
    "        sorted_product_sales_list = [row.asDict() for row in sorted_product_sales.collect()]\n",
    "    \n",
    "    # Insert into MongoDB (assuming you have established a connection)\n",
    "    # Replace 'db.highest_sales_revenue_products' with your MongoDB collection\n",
    "    # db.highest_sales_revenue_products.insert_many(sorted_product_sales_list)\n",
    "        \n",
    "         # Insert into MongoDB if the list is not empty\n",
    "        if sorted_product_sales_list:\n",
    "          coll_high_revenue.insert_many(sorted_product_sales_list)\n",
    "          sorted_product_sales.show()\n",
    "          logger.info(f\"Batch {batch_id} processed and inserted into MongoDB\")\n",
    "        else:\n",
    "          logger.info(f\"Batch {batch_id} processed but no high-value customers found\")\n",
    "    # Insert into MongoDB (assuming you have established a connection)\n",
    "    # Replace 'db.high_value_customer_habits' with your MongoDB collection\n",
    "  \n",
    "   except Exception as e:\n",
    "        logger.error(f\"Error processing batch {batch_id}: {e}\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4214b-8293-4bea-9e5c-691f6abd9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.java_gateway:Callback Server Starting\n",
      "INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 37125)\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "ERROR:__main__:Error processing batch 0: name 'popular_categories_list' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Write the streaming DataFrame to the console using the function\n",
    "try:\n",
    "   # Write the streaming DataFrame to the console using the function\n",
    "   query_highest_sales_revenue = customer_streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(process_highest_sales_revenue) \\\n",
    "    .start()\n",
    "    # Await termination\n",
    "   query_highest_sales_revenue.awaitTermination()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in streaming query: {e}\")\n",
    "finally:\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
